{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9513b82c",
      "metadata": {
        "id": "9513b82c"
      },
      "source": [
        "# Introduction to GrabGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bd7ba4a",
      "metadata": {
        "id": "2bd7ba4a"
      },
      "source": [
        "## Learning Objectives\n",
        "1. Setting up the required variables to call the endpoint GrabGPT API\n",
        "2. Making chat compltion calls to library\n",
        "3. Handling intermittent network and rate limit issues gracefully"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "80164b95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80164b95",
        "outputId": "ec2304a2-91c6-4edd-9e31-49a3e02ad13e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-core\n",
            "  Downloading langchain_core-0.3.12-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.4-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting openai<2.0.0,>=1.52.0 (from langchain-openai)\n",
            "  Downloading openai-1.52.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (6.0.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.125 (from langchain-core)\n",
            "  Downloading langsmith-0.1.136-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (4.12.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.15.2)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.125->langchain-core)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain-core)\n",
            "  Downloading orjson-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain-core)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (1.7.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.52.0->langchain-openai)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (4.66.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.52.0->langchain-openai) (1.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading langchain_openai-0.2.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.12-py3-none-any.whl (407 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.7/407.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.4-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.136-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.52.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, orjson, jsonpointer, jiter, h11, tiktoken, requests-toolbelt, jsonpatch, httpcore, httpx, openai, langsmith, langchain-core, langchain-text-splitters, langchain-openai, langchain\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.4 langchain-core-0.3.12 langchain-openai-0.2.3 langchain-text-splitters-0.3.0 langsmith-0.1.136 openai-1.52.0 orjson-3.10.9 python-dotenv-1.0.1 requests-toolbelt-1.0.0 tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "%pip install python-dotenv langchain-openai langchain-core langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0fee8994-ca19-4fa0-b586-c8b94f4af0de",
      "metadata": {
        "tags": [],
        "id": "0fee8994-ca19-4fa0-b586-c8b94f4af0de"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "03fa41e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03fa41e2",
        "outputId": "8cea24c3-9681-4990-f9cd-def472ab7193"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Ensure environment variables are loaded before accessing them\n",
        "load_dotenv(\"template.env\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1acbce11",
      "metadata": {
        "id": "1acbce11"
      },
      "source": [
        "### Set the API Key environment\n",
        "- gpt-4\n",
        "- gpt-3.5-turbo\n",
        "- langsmith\n",
        "- Additional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3db16bb9",
      "metadata": {
        "id": "3db16bb9"
      },
      "outputs": [],
      "source": [
        "# GPT 4\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "gpt = ChatOpenAI(model='gpt-4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ddf6cd24",
      "metadata": {
        "id": "ddf6cd24"
      },
      "outputs": [],
      "source": [
        "# GPT 3\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "gpt3 = ChatOpenAI(model='gpt-3.5-turbo')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b4ce8a9",
      "metadata": {
        "id": "4b4ce8a9"
      },
      "source": [
        "### Introduction to LangSmith\n",
        "\n",
        "LangSmith is a tool in the Langchain ecosystem designed to help monitor, evaluate, and debug language models more effectively. It provides the infrastructure for logging interactions, running tests, and tracking metrics to ensure optimal model performance.\n",
        "\n",
        "We will be covering this topic in depth in future sessions!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "e187e4e1",
      "metadata": {
        "id": "e187e4e1"
      },
      "outputs": [],
      "source": [
        "# Last step: LangSmith\n",
        "tracing = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
        "langsmith = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "\n",
        "# Let's you trace everything that is going on in this codespace."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0847e38",
      "metadata": {
        "id": "d0847e38"
      },
      "source": [
        "# What is GPT?\n",
        "\n",
        "GPT refers to a suite of powerful language models developed by OpenAI, such as **GPT-3**, **GPT-4**, and **GPT-4 with Vision**. These models can perform a wide variety of tasks, including text generation, conversation, translation, and image processing (in the case of GPT-4 with Vision).\n",
        "\n",
        "Through OpenAI's API, users can access these models directly via OpenAI’s platform, bypassing the need for third-party integrations like Azure. With an **OpenAI API key**, you have direct access to the latest models OpenAI offers.\n",
        "\n",
        "While our focus for this training is on OpenAI’s models, which are the most widely used and well-documented, the principles for interacting with other large language models (LLMs) like Meta’s LLaMA or Anthropic’s Claude are largely similar.\n",
        "\n",
        "## Available Models through OpenAI API:\n",
        "\n",
        "| Model Name                      | Description                               |\n",
        "|----------------------------------|-------------------------------------------|\n",
        "| **gpt-3.5-turbo**                | A highly efficient variant of GPT-3.5, great for most conversational tasks. |\n",
        "| **gpt-4**                        | The latest iteration of OpenAI’s powerful GPT series, offering enhanced understanding and reasoning abilities. |\n",
        "| **gpt-4-32k**                    | A larger version of GPT-4 with the ability to process longer context windows. |\n",
        "| **gpt-4 with Vision**            | Allows GPT-4 to process both text and images, useful for tasks that combine visual and textual inputs. |\n",
        "| **text-embedding-ada-002**       | A model specialized for creating embeddings for tasks like text similarity, search, and clustering. |\n",
        "| **Whisper**                      | OpenAI’s speech recognition model for transcribing and translating audio. |\n",
        "| **DALL·E**                       | OpenAI’s image generation model, capable of generating images from textual descriptions. |\n",
        "\n",
        "For a comprehensive list of models and capabilities available through OpenAI’s API, you can refer to the [OpenAI API documentation](https://platform.openai.com/docs).\n",
        "\n",
        "---\n",
        "\n",
        "This version reflects the use of the OpenAI API key for direct access to OpenAI's models, rather than relying on Azure or other providers. Let me know if you'd like further adjustments!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "69a4ed35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69a4ed35",
        "outputId": "4df07b10-110a-4927-af23-34e1087b7cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Me encanta el helado, la salsa Lao Gan Ma y John Cena' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 30, 'total_tokens': 46, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-1a674221-3fa8-490d-8bbd-e5f543cf7573-0' usage_metadata={'input_tokens': 30, 'output_tokens': 16, 'total_tokens': 46, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n",
            "Me encanta el helado, la salsa Lao Gan Ma y John Cena\n"
          ]
        }
      ],
      "source": [
        "# gpt 4\n",
        "# Run once can le!\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Translate the following from English into Spanish\"),\n",
        "    HumanMessage(content=\"I love ice cream, lao gan ma and john cena\")\n",
        "]\n",
        "\n",
        "message = gpt.invoke(messages)\n",
        "print(message)\n",
        "print(message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26dc383b",
      "metadata": {
        "id": "26dc383b"
      },
      "source": [
        "### Tokens\n",
        "The token limit determines how much information can be handled in a single interaction. For large documents or complex tasks, a higher token limit allows for more extensive input and output, while lower limits mean shorter interactions.\n",
        "\n",
        "Why is this important?\n",
        "The token limit determines how much information can be handled in a single interaction. For large documents or complex tasks, a higher token limit allows for more extensive input and output, while lower limits mean shorter interactions.\n",
        "\n",
        "GPT-4 (8k context model): This version has a maximum token limit of 8,192 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "401f78dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "401f78dd",
        "outputId": "d67db734-ae82-4dfa-e2be-138584a515ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.messages.ai.AIMessage"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.messages.ai.AIMessage</b><br/>def __init__(content: Union[str, list[Union[str, dict]]], **kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/langchain_core/messages/ai.py</a>Message from an AI.\n",
              "\n",
              "AIMessage is returned from a chat model as a response to a prompt.\n",
              "\n",
              "This message represents the output of the model and consists of both\n",
              "the raw output as returned by the model together standardized fields\n",
              "(e.g., tool calls, usage metadata) added by the LangChain framework.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 141);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "type(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ca539ff",
      "metadata": {
        "id": "4ca539ff"
      },
      "outputs": [],
      "source": [
        "# gpt 3.5 turbo\n",
        "messages = [\n",
        "    SystemMessage(content=\"Translate the following from English into Spanish\"),\n",
        "    HumanMessage(content=\"I love ice cream, lao gan ma and john cena\")\n",
        "]\n",
        "\n",
        "message = gpt3.invoke(messages)\n",
        "print(message)\n",
        "print(message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1643b88d",
      "metadata": {
        "id": "1643b88d"
      },
      "source": [
        "# Try it out yourself!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "11873c8c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11873c8c",
        "outputId": "37967cad-1802-497c-d1b2-8176ffd4fb95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=\"A typical Singaporean breakfast usually consists of a variety of dishes influenced by the multicultural heritage of the country. Some popular breakfast options include:\\n\\n1. Kaya Toast: A traditional Singaporean breakfast staple, kaya toast is a simple yet delicious dish made with toasted bread spread with kaya (a sweet coconut and egg jam) and a slab of butter.\\n\\n2. Soft-Boiled Eggs: Soft-boiled eggs are often served with a dash of soy sauce and white pepper. Dipping the kaya toast into the soft-boiled eggs is a common practice among locals.\\n\\n3. Nasi Lemak: A fragrant coconut rice dish served with various accompaniments such as fried fish, fried chicken, sambal (spicy chili paste), peanuts, and hard-boiled eggs.\\n\\n4. Roti Prata: A South Indian-influenced dish made of crispy, flaky flatbread usually served with a side of curry for dipping.\\n\\n5. Mee Rebus: A Malay noodle dish served in a thick and spicy gravy topped with boiled egg, fried tofu, and bean sprouts.\\n\\nThese are just a few examples of the diverse breakfast options you can find in Singapore. The country's food culture is a delicious blend of Chinese, Malay, Indian, and other culinary influences, making breakfast a delightful and flavorful experience for locals and visitors alike.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 274, 'prompt_tokens': 28, 'total_tokens': 302, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-87e1acad-29a2-4b0c-9e20-ef62d9e7ccf4-0' usage_metadata={'input_tokens': 28, 'output_tokens': 274, 'total_tokens': 302, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n",
            "{'token_usage': {'completion_tokens': 274, 'prompt_tokens': 28, 'total_tokens': 302, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "1) Ask chatgpt for Singapore's most popular breakfast meal!\n",
        "2) Take note of the tokens used\n",
        "3) Find out the cost!\n",
        "'''\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a Singaporean Foodie Expert.\"),\n",
        "    HumanMessage(content=\"What is a Singaporean breakfast like?\"),\n",
        "]\n",
        "\n",
        "output = gpt.invoke(messages)\n",
        "print(output)\n",
        "print(output.response_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4369aa80",
      "metadata": {
        "id": "4369aa80"
      },
      "outputs": [],
      "source": [
        "# Content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ab72657",
      "metadata": {
        "id": "7ab72657"
      },
      "outputs": [],
      "source": [
        "# Token usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "253b232f",
      "metadata": {
        "id": "253b232f"
      },
      "source": [
        "## Prompt Engineering & Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d9b44198",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9b44198",
        "outputId": "aec29db5-076e-46c7-923e-dd02cfbba3f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Prompt from Template-----\n",
            "messages=[HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate # Mimic what you will see when using ChatGPT UI\n",
        "\n",
        "# PART 1: Create a ChatPromptTemplate using a template string\n",
        "print(\"-----Prompt from Template-----\")\n",
        "template = \"Tell me a joke about {topic}\"\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = gpt3.invoke(prompt)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwTvGUDgVTPU",
        "outputId": "aa7350a9-5875-40b1-fa5c-b403255827c8"
      },
      "id": "hwTvGUDgVTPU",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Why was the cat sitting on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 13, 'total_tokens': 33, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-26a534c9-5d59-4ca9-8b90-e98120898ed7-0' usage_metadata={'input_tokens': 13, 'output_tokens': 20, 'total_tokens': 33, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d364f17",
      "metadata": {
        "id": "2d364f17"
      },
      "source": [
        "#### You will notice there is alot of room for dynamic changes using a prompt template as compared to using the standard chat template above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15d6310d",
      "metadata": {
        "id": "15d6310d"
      },
      "outputs": [],
      "source": [
        "# PART 2: Prompt with Multiple Placeholders\n",
        "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
        "template_multiple = \"\"\"You are a helpful assistant.\n",
        "Human: Tell me a {adjective} short story about a {animal}.\n",
        "Assistant:\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "91a8b020",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91a8b020",
        "outputId": "9a26358c-da71-469b-e7f5-31aa358b9cb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with System and Human Messages (Tuple) -----\n",
            "\n",
            "content=\"1. Why did the lawyer bring a ladder to court? Because he heard the case was going to be heard on a higher level!\\n\\n2. How many lawyers does it take to change a lightbulb? Three - one to climb the ladder, one to shake it, and one to sue the ladder company for faulty equipment!\\n\\n3. Why don't lawyers play hide and seek? Because good luck finding one who isn't billing hours while hiding!\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 27, 'total_tokens': 117, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-99e5be79-7c8d-45e5-8060-3bc2d4ace7da-0' usage_metadata={'input_tokens': 27, 'output_tokens': 90, 'total_tokens': 117, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "# PART 3: Prompt with System and Human Messages (Using Tuples)\n",
        "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
        "\n",
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "]\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "prompt = prompt_template.invoke({\"topic\":\"lawyers\", \"joke_count\":\"3\"})\n",
        "result = gpt3.invoke(prompt)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d938831e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d938831e",
        "outputId": "a5fb285e-cc3f-44ab-aa5a-02ee9fcc60f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='To solve the integral \\\\(\\\\int_0^2 (3x^2 - 2x + 1) \\\\, dx\\\\), we need to integrate each term separately and then evaluate the definite integral from 0 to 2.\\n\\nThe integral of \\\\(3x^2\\\\) with respect to \\\\(x\\\\) is \\\\(x^3\\\\).  \\nThe integral of \\\\(-2x\\\\) with respect to \\\\(x\\\\) is \\\\(-x^2\\\\).  \\nThe integral of \\\\(1\\\\) with respect to \\\\(x\\\\) is \\\\(x\\\\).\\n\\nNow, we can integrate each term:\\n\\\\[\\n\\\\int_0^2 (3x^2 - 2x + 1) \\\\, dx = \\\\left[ x^3 - x^2 + x \\\\right]_0^2\\n\\\\]\\n\\nNow, we can evaluate the definite integral:\\n\\\\[\\n\\\\left[ 2^3 - 2^2 + 2 \\\\right] - \\\\left[ 0^3 - 0^2 + 0 \\\\right]\\n\\\\]\\n\\\\[\\n= (8 - 4 + 2) - (0 - 0 + 0)\\n\\\\]\\n\\\\[\\n= 6\\n\\\\]\\n\\nTherefore, the value of the definite integral \\\\(\\\\int_0^2 (3x^2 - 2x + 1) \\\\, dx\\\\) is 6.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 285, 'prompt_tokens': 46, 'total_tokens': 331, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-292fecea-7575-44ae-ad67-d520733ae956-0' usage_metadata={'input_tokens': 46, 'output_tokens': 285, 'total_tokens': 331, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "# Try it out yourself\n",
        "\"\"\"\n",
        "Write a Python code snippet using ChatPromptTemplate to:\n",
        "\n",
        "1) Create the system and human messages using tuples.\n",
        "2) The system message should say: \"You are a calculus expert tutor.\"\n",
        "3) The human message should say: \"Help me solve {problem_count} calculus problems.\"\n",
        "4) Use problem_count = \"\\int_0^2 (3x^2 - 2x + 1) \\, dx\" as an input.\n",
        "5) Invoke the prompt and print the response from the model.\n",
        "\"\"\"\n",
        "messages = [\n",
        "    (\"system\", \"You are a calculus expert tutor.\"),\n",
        "    (\"human\", \"Help me solve {problem_count} calculus problems.\"),\n",
        "]\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "prompt = prompt_template.invoke({\"problem_count\":\"\\int_0^2 (3x^2 - 2x + 1) \\, dx\"})\n",
        "result = gpt3.invoke(prompt)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8133dfbd",
      "metadata": {
        "id": "8133dfbd"
      },
      "source": [
        "### Additional\n",
        "\n",
        "From lazy prompt to detailed prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e05917b0",
      "metadata": {
        "id": "e05917b0"
      },
      "outputs": [],
      "source": [
        "# We can take prompts that were pre made by people!\n",
        "from langchain import hub\n",
        "prompt = hub.pull(\"hardkothari/prompt-maker\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec3c9dc",
      "metadata": {
        "id": "8ec3c9dc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(new_prompt.messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02f625f7",
      "metadata": {
        "id": "02f625f7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc90f635",
      "metadata": {
        "id": "dc90f635"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a0d55772",
      "metadata": {
        "id": "a0d55772"
      },
      "source": [
        "## Few-Shot Prompt Template Example\n",
        "In this section, we'll learn how to create a simple prompt template that helps guide the model by providing example inputs and outputs. This technique, known as few-shotting, involves showing the model a few examples to help it understand the task better. It is a powerful way to improve the quality of the generated output, especially when the task is complex or context-dependent.\n",
        "\n",
        "A few-shot prompt template can be built from a fixed set of examples or can be dynamically constructed using an Example Selector class, which is responsible for selecting relevant examples from a pre-defined set based on the query.\n",
        "\n",
        "## Parameter Explanations\n",
        "\n",
        "#### Prompt Template: A framework that structures your prompt and integrates a set of few-shot examples to guide the model's behavior.\n",
        "\n",
        "#### Few-Shotting: Providing a series of example inputs and outputs to the model in the prompt. This helps the model generate better responses by mimicking the patterns in the examples.\n",
        "\n",
        "If you want to read more about [Few-Shot-Prompting Papers](https://arxiv.org/abs/2005.14165)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9a49079",
      "metadata": {
        "id": "a9a49079"
      },
      "source": [
        "## Difference Between Zero-Shot, One-Shot, and Few-Shot Learning\n",
        "\n",
        "- **Zero-Shot Learning**: In zero-shot learning, the model is able to perform a task without having seen any examples or prior data for that specific task. It relies on knowledge transfer from other tasks or context provided by the model.\n",
        "\n",
        "- **One-Shot Learning**: In one-shot learning, the model is trained on only one example of each task or class and is expected to generalize well enough to perform accurately on new, unseen data.\n",
        "\n",
        "- **Few-Shot Learning**: In few-shot learning, the model is trained on a small number of examples (usually a handful) for each class or task, and it uses this limited data to make predictions or perform the task on new examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88bc582d",
      "metadata": {
        "id": "88bc582d"
      },
      "source": [
        "# Zero-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fbce165",
      "metadata": {
        "id": "0fbce165"
      },
      "outputs": [],
      "source": [
        "# Zero-shot example: Sentiment analysis (classification task with no prior examples)\n",
        "\n",
        "# System message defines the assistant's role\n",
        "content=\"You are a helpful assistant. Who is great at picking up the nuances in a sentence and direct in the way you respond\"\n",
        "\n",
        "# Human message asks the model to classify the sentiment of the sentence\n",
        "content=\"Classify the following sentence as positive, negative, or neutral: 'The product quality is excellent and I love it!'\"\n",
        "\n",
        "# Send the conversation to the model\n",
        "\n",
        "\n",
        "# Output the model's response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6c81a9d",
      "metadata": {
        "id": "f6c81a9d"
      },
      "source": [
        "## This can be problematic when the task is very ambiguous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3740ad48",
      "metadata": {
        "id": "3740ad48"
      },
      "outputs": [],
      "source": [
        "system_message = SystemMessage(content=\"You are a helpful assistant. Who is great at picking up the nuances in a sentence and direct in the way you respond.\")\n",
        "\n",
        "human_message= HumanMessage(content=\"Classify the following sentence as positive, negative, or neutral:\\\n",
        "    'The product works well most of the time, but there are moments when it suddenly stops working,\\\n",
        "    which can be frustrating. However, I think it's a decent option overall, and the design is nice,\\\n",
        "    though I expected a bit more durability for the price.'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "374eeda3",
      "metadata": {
        "id": "374eeda3"
      },
      "source": [
        "# One shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0923631f",
      "metadata": {
        "id": "0923631f"
      },
      "outputs": [],
      "source": [
        "# System message defines the assistant's role\n",
        "\n",
        "\n",
        "# Human message provides a one-shot example of English-to-French translation and asks for a new translation\n",
        "\n",
        "\"Translate the following sentence from English to French:\\n\"\n",
        "\"Example: 'I love data science.' => 'J'adore la science des données.'\\n\"\n",
        "\"Now translate: 'Data is the new oil.'\"\n",
        "\n",
        "\n",
        "# Send the conversation to the model\n",
        "\n",
        "\n",
        "# Output the model's response\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "968f8e17-7b59-48ec-8543-bae5a15a3899",
      "metadata": {
        "id": "968f8e17-7b59-48ec-8543-bae5a15a3899"
      },
      "source": [
        "## Few Shots\n",
        "\n",
        "One of the most effective ways to improve model performance is to give a model examples of what you want it to do. The technique of adding example inputs and expected outputs to a model prompt is known as \"few-shot prompting\". The technique is based on the Language Models are Few-Shot Learners paper. There are a few things to think about when doing few-shot prompting:\n",
        "\n",
        "How are examples generated?\n",
        "How many examples are in each prompt?\n",
        "How are examples selected at runtime?\n",
        "How are examples formatted in the prompt?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "fa21146b",
      "metadata": {
        "id": "fa21146b"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import(\n",
        "    ChatPromptTemplate,\n",
        "    FewShotChatMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f7e466e1",
      "metadata": {
        "id": "f7e466e1"
      },
      "outputs": [],
      "source": [
        "# We can create some examples to show our AI what we want\n",
        "examples = [\n",
        "    {\"input\":\"2+2\", \"output\": \"4\"},\n",
        "    {\"input\":\"2+3\", \"output\": \"5\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e642aa2c",
      "metadata": {
        "id": "e642aa2c"
      },
      "outputs": [],
      "source": [
        "#Create an example_prompt\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "7b556f2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b556f2d",
        "outputId": "49f6d4a4-5510-4aee-aa3e-30a4aaea6d0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: 2+2\n",
            "AI: 4\n",
            "Human: 2+3\n",
            "AI: 5\n"
          ]
        }
      ],
      "source": [
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt = example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "print(few_shot_prompt.format())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "21b35de6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21b35de6",
        "outputId": "c68b694a-5c1f-46e5-a9ee-35ebfc7f302c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('name', None)\n",
            "('examples', [{'input': '2+2', 'output': '4'}, {'input': '2+3', 'output': '5'}])\n",
            "('example_selector', None)\n",
            "('input_variables', [])\n",
            "('optional_variables', [])\n",
            "('input_types', {})\n",
            "('output_parser', None)\n",
            "('partial_variables', {})\n",
            "('metadata', None)\n",
            "('tags', None)\n",
            "('example_prompt', ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})]))\n"
          ]
        }
      ],
      "source": [
        "for item in few_shot_prompt:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "6fb71dab",
      "metadata": {
        "id": "6fb71dab"
      },
      "outputs": [],
      "source": [
        "system_message=\"You are a wonderous wizard of math.\"\n",
        "human_template=\"{input}\"\n",
        "\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "final_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        system_message_prompt, #System message to use\n",
        "        few_shot_prompt,       #Examples to take\n",
        "        human_message_prompt,  #Prompts\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "d18877bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d18877bc",
        "outputId": "44b84a74-1ee4-4056-9d2f-c9c094f7a54e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='The square root of 9 is 3.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 53, 'total_tokens': 63, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-d21d8dc9-85d4-48f5-b5bb-51077891ed90-0' usage_metadata={'input_tokens': 53, 'output_tokens': 10, 'total_tokens': 63, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "# We will use chain here each item is called a runnable,\n",
        "# we will dive deeper in the future session\n",
        "\n",
        "chain = final_prompt | gpt3\n",
        "\n",
        "results = chain.invoke(\"What's the square root of 9?\")\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "06e580f1",
      "metadata": {
        "id": "06e580f1"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
        "        \"answer\": \"\"\"\n",
        "            Are follow up questions needed here: Yes.\n",
        "            Follow up: How old was Muhammad Ali when he died?\n",
        "            Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
        "            Follow up: How old was Alan Turing when he died?\n",
        "            Intermediate answer: Alan Turing was 41 years old when he died.\n",
        "            So the final answer is: Muhammad Ali\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"When was the founder of craigslist born?\",\n",
        "        \"answer\": \"\"\"\n",
        "            Are follow up questions needed here: Yes.\n",
        "            Follow up: Who was the founder of craigslist?\n",
        "            Intermediate answer: Craigslist was founded by Craig Newmark.\n",
        "            Follow up: When was Craig Newmark born?\n",
        "            Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
        "            So the final answer is: December 6, 1952\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Who was the maternal grandfather of George Washington?\",\n",
        "        \"answer\": \"\"\"\n",
        "            Are follow up questions needed here: Yes.\n",
        "            Follow up: Who was the mother of George Washington?\n",
        "            Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
        "            Follow up: Who was the father of Mary Ball Washington?\n",
        "            Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
        "            So the final answer is: Joseph Ball\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n",
        "        \"answer\": \"\"\"\n",
        "            Are follow up questions needed here: Yes.\n",
        "            Follow up: Who is the director of Jaws?\n",
        "            Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
        "            Follow up: Where is Steven Spielberg from?\n",
        "            Intermediate Answer: The United States.\n",
        "            Follow up: Who is the director of Casino Royale?\n",
        "            Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
        "            Follow up: Where is Martin Campbell from?\n",
        "            Intermediate Answer: New Zealand.\n",
        "            So the final answer is: No\n",
        "        \"\"\",\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1b43404",
      "metadata": {
        "id": "f1b43404"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Define the structure for individual examples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0f99cc7",
      "metadata": {
        "id": "f0f99cc7"
      },
      "outputs": [],
      "source": [
        "# Invoke the example prompt with the first example\n",
        "print(example_prompt.invoke(examples[0]))\n",
        "print(example_prompt.invoke(examples[0]).to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33fd84a2",
      "metadata": {
        "id": "33fd84a2"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import FewShotPromptTemplate\n",
        "\n",
        "prompt = FewShotPromptTemplate(\n",
        "\n",
        ")\n",
        "\n",
        "print(\n",
        "    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string()\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}