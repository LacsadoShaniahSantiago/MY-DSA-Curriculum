{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZooFTWZo8eX"
      },
      "source": [
        "# DSA Deep Learning [3] - Hypertuning Our CNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import statements\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import io\n",
        "import base64\n",
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import display, HTML, Javascript\n",
        "from google.colab import output, files\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "3HC-1Ua0Z33_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5 Different Approach to Improve Accuracy and Low Functionality\n",
        "1. Bigger Dataset\n",
        "2. More Algorithms\n",
        "3. Augmentation\n",
        "4. Pre-processing\n",
        "5. More training"
      ],
      "metadata": {
        "id": "9iWUpeZiOZcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load CSV files\n",
        "train_csv_path = \"train.csv\"\n",
        "val_csv_path = \"fer2013.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "val_df = pd.read_csv(val_csv_path)"
      ],
      "metadata": {
        "id": "bfg8ijaHRrhs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Info\n",
        "print(train_df.head())\n",
        "print(train_df.columns)\n",
        "print(val_df.head())\n",
        "print(val_df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f03HPfTSBXG",
        "outputId": "77da4f20-272f-4ebc-e0fa-2843802f42e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   emotion                                             pixels\n",
            "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
            "1        0  151 150 147 155 148 133 111 140 170 174 182 15...\n",
            "2        2  231 212 156 164 174 138 161 173 182 200 106 38...\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
            "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...\n",
            "Index(['emotion', 'pixels'], dtype='object')\n",
            "   emotion                                             pixels     Usage\n",
            "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
            "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
            "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
            "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n",
            "Index(['emotion', 'pixels', 'Usage'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Function for Data Injestion\n",
        "def load_data(csv_path):\n",
        "  df = pd.read_csv(csv_path)\n",
        "  X = np.array([np.fromstring(pixel_str, dtype=float, sep=' ') for pixel_str in df['pixels']])\n",
        "  X = X.reshape(-1,48,48,1)\n",
        "  X = np.repeat(X, 3, axis=-1)\n",
        "  X = X / 255.0\n",
        "  y = to_categorical(df['emotion'])\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "mbGTR6kQSf75"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Injestion\n",
        "X_train, y_train = load_data(train_csv_path)\n",
        "X_val, y_val = load_data(val_csv_path)"
      ],
      "metadata": {
        "id": "2em-XZ91Zyuc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **#1 Issue on Machine Learning: Overfitting**\n",
        "Overspecialing on more weighted attributes that creates a model that cannot generalise the data and fits too closely to the training dataset"
      ],
      "metadata": {
        "id": "P_8hl3f8ap0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Generator - Selecting a standardized collection for balance class weight\n",
        "def create_generator(X, y, batch_size = 64, shuffle=True):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(buffer_size=1024)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "3y8yD9fIZ6YI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "train_generator = create_generator(X_train, y_train, batch_size=batch_size, shuffle=True)\n",
        "val_generator = create_generator(X_val, y_val, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "LJNP8q8LaWzK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Generalisation\n",
        "y_train_labels = np.argmax(y_train, axis=1)\n",
        "\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train_labels),\n",
        "    y=y_train_labels\n",
        ")\n",
        "\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "#Remove irrelevant data (i.e. data on hair features that does not contribute to facial emotions)\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range = 30,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    horizontal_flip = True,\n",
        "    fill_mode = 'nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "aw91-BTwczny"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = MobileNetV2(input_shape=(48, 48, 3), include_top=False, weights=\"imagenet\")\n",
        "\n",
        "for layers in base_model.layers:\n",
        "  layers.trainable = False\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation=\"relu\", kernel_regularizer=l2(0.01))(x)\n",
        "#Kernel regularizer, an algorithm/technique\n",
        "x = Dropout(0.5)(x)\n",
        "#Dropout, regularization technique, randomly drops out certain percentage of neurons(data)\n",
        "#in each layer during training process to achieve better generalisation on unseen data\n",
        "predictions = Dense(7, activation=\"softmax\")(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "#Fine Tunning for Precision\n",
        "for layers in base_model.layers[-20:]:\n",
        "  layers.trainable = True\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nkw143efNkb",
        "outputId": "dbfaf159-b4b4-41c7-b9c8-3743a916ff6b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-3240e32f924f>:1: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(input_shape=(48, 48, 3), include_top=False, weights=\"imagenet\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reduces learning rate when no improvement in validation loss, validation loss plateau\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, min_lr=5)\n",
        "\n",
        "#Stops training process when validation loss plateau\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
        "\n",
        "#Saves model with best validation loss\n",
        "checkpoint = ModelCheckpoint(\"emotion_recognition_model_advanced.keras\", monitor=\"val_loss\", save_best_only=True)\n",
        "\n",
        "#Combines the 3 callbacks above\n",
        "callbacks = [reduce_lr, early_stopping, checkpoint]"
      ],
      "metadata": {
        "id": "WdMdnbtahv1F"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=20,\n",
        "    validation_data=val_generator,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FC1Ee2L6jihP",
        "outputId": "a3a3128a-12c9-43f7-a248-e7ebabf877c3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 55ms/step - accuracy: 0.2034 - loss: 4.3894 - val_accuracy: 0.3113 - val_loss: 3.7211 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.3051 - loss: 3.4849 - val_accuracy: 0.3597 - val_loss: 3.1308 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.3522 - loss: 2.9421 - val_accuracy: 0.4283 - val_loss: 2.6304 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.3976 - loss: 2.4856 - val_accuracy: 0.4536 - val_loss: 2.3240 - learning_rate: 1.0000e-04\n",
            "Epoch 5/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.4348 - loss: 2.1581 - val_accuracy: 0.4967 - val_loss: 2.0463 - learning_rate: 1.0000e-04\n",
            "Epoch 6/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.4689 - loss: 1.8978 - val_accuracy: 0.5273 - val_loss: 1.8267 - learning_rate: 1.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.5133 - loss: 1.6751 - val_accuracy: 0.5661 - val_loss: 1.6454 - learning_rate: 1.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.5519 - loss: 1.4988 - val_accuracy: 0.6138 - val_loss: 1.4557 - learning_rate: 1.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.5793 - loss: 1.3520 - val_accuracy: 0.6300 - val_loss: 1.3563 - learning_rate: 1.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 23ms/step - accuracy: 0.6184 - loss: 1.2104 - val_accuracy: 0.6628 - val_loss: 1.2444 - learning_rate: 1.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 19ms/step - accuracy: 0.6525 - loss: 1.0826 - val_accuracy: 0.6810 - val_loss: 1.1642 - learning_rate: 1.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 26ms/step - accuracy: 0.7060 - loss: 0.9506 - val_accuracy: 0.7223 - val_loss: 1.0540 - learning_rate: 1.0000e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.7391 - loss: 0.8465 - val_accuracy: 0.7275 - val_loss: 1.0124 - learning_rate: 1.0000e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.7647 - loss: 0.7517 - val_accuracy: 0.7590 - val_loss: 0.9542 - learning_rate: 1.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.8058 - loss: 0.6514 - val_accuracy: 0.7862 - val_loss: 0.8870 - learning_rate: 1.0000e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.8219 - loss: 0.5892 - val_accuracy: 0.8199 - val_loss: 0.7951 - learning_rate: 1.0000e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.8515 - loss: 0.5185 - val_accuracy: 0.8308 - val_loss: 0.7654 - learning_rate: 1.0000e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.8670 - loss: 0.4740 - val_accuracy: 0.8467 - val_loss: 0.7389 - learning_rate: 1.0000e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.8914 - loss: 0.4075 - val_accuracy: 0.8532 - val_loss: 0.7116 - learning_rate: 1.0000e-04\n",
            "Epoch 20/20\n",
            "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.9083 - loss: 0.3609 - val_accuracy: 0.8639 - val_loss: 0.6965 - learning_rate: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardized Model\n",
        "model_path=\"emotion_recognition_model_advanceed.keras\""
      ],
      "metadata": {
        "id": "yJcl9EoDmttZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary for emotion labels based on FER2013 class order\n",
        "emotion_labels = {\n",
        "    0: \"Angry\",\n",
        "    1: \"Disgust\",\n",
        "    2: \"Fear\",\n",
        "    3: \"Happy\",\n",
        "    4: \"Sad\",\n",
        "    5: \"Surprise\",\n",
        "    6: \"Neutral\"\n",
        "}\n",
        "\n",
        "# Updated predict_emotion function to handle multiple faces\n",
        "def predict_emotion(frame, model):\n",
        "    # Convert to grayscale for face detection\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Load the face detection model (Haar Cascade)\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    # Detect multiple faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(48, 48))\n",
        "\n",
        "    # Process each detected face\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Extract the face region from the frame\n",
        "        face = frame[y:y+h, x:x+w]\n",
        "\n",
        "        # Resize face region to 48x48, the input size expected by the model\n",
        "        face_resized = cv2.resize(face, (48, 48))\n",
        "\n",
        "        # Preprocess face (normalize and add batch dimension)\n",
        "        face_array = np.expand_dims(face_resized, axis=0) / 255.0  # Scale pixel values to [0, 1]\n",
        "\n",
        "        # Predict emotion\n",
        "        emotion_prediction = model.predict(face_array)\n",
        "        emotion = np.argmax(emotion_prediction)  # Get the emotion class with the highest probability\n",
        "\n",
        "        # Draw a circle around the face and add the emotion label\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        emotion_label = emotion_labels[emotion]  # Map the predicted emotion index to label\n",
        "        cv2.putText(frame, emotion_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    return frame\n"
      ],
      "metadata": {
        "id": "closxhY9fpik"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript code to start the live webcam feed and capture image upon button click\n",
        "def start_webcam_feed():\n",
        "    js = \"\"\"\n",
        "    <script>\n",
        "        let videoElement = null;\n",
        "        let stream = null;\n",
        "\n",
        "        async function startVideo() {\n",
        "            if (!videoElement) {\n",
        "                videoElement = document.createElement('video');\n",
        "                videoElement.setAttribute('autoplay', '');\n",
        "                videoElement.setAttribute('playsinline', '');\n",
        "                document.body.appendChild(videoElement);\n",
        "                stream = await navigator.mediaDevices.getUserMedia({ video: true })\n",
        "                  .catch(err => {\n",
        "                      console.error('Webcam not accessible:', err);\n",
        "                      alert('Webcam not accessible. You can upload an image instead.');\n",
        "                  });\n",
        "                if (stream) {\n",
        "                    videoElement.srcObject = stream;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "        async function capturePhoto() {\n",
        "            if (!videoElement) {\n",
        "                alert(\"Webcam is not active!\");\n",
        "                return;\n",
        "            }\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = videoElement.videoWidth;\n",
        "            canvas.height = videoElement.videoHeight;\n",
        "            canvas.getContext('2d').drawImage(videoElement, 0, 0);\n",
        "\n",
        "            // Stop video feed\n",
        "            stream.getTracks().forEach(track => track.stop());\n",
        "            videoElement.remove();\n",
        "            videoElement = null;\n",
        "\n",
        "            // Convert the photo to base64 and send to Python\n",
        "            const dataUrl = canvas.toDataURL('image/jpeg');\n",
        "            google.colab.kernel.invokeFunction('notebook.get_webcam_image', [dataUrl], {});\n",
        "        }\n",
        "\n",
        "        // Add the start and capture buttons to the DOM\n",
        "        const startButton = document.createElement('button');\n",
        "        startButton.innerHTML = 'Start Webcam Feed';\n",
        "        startButton.onclick = startVideo;\n",
        "        document.body.appendChild(startButton);\n",
        "\n",
        "        const captureButton = document.createElement('button');\n",
        "        captureButton.innerHTML = 'Capture Photo';\n",
        "        captureButton.onclick = capturePhoto;\n",
        "        document.body.appendChild(captureButton);\n",
        "    </script>\n",
        "    \"\"\"\n",
        "    display(HTML(js))\n",
        "\n",
        "# Callback function to receive the captured image in Python\n",
        "def get_webcam_image(dataUrl):\n",
        "    img_data = base64.b64decode(dataUrl.split(\",\")[1])\n",
        "    img = Image.open(io.BytesIO(img_data))\n",
        "    img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "    processed_img = predict_emotion(img, model)\n",
        "    cv2_imshow(processed_img)\n",
        "\n",
        "# Register the callbacks\n",
        "output.register_callback('notebook.get_webcam_image', get_webcam_image)\n",
        "\n",
        "# Initialize the webcam feed, buttons, and file upload option\n",
        "start_webcam_feed()\n",
        "\n"
      ],
      "metadata": {
        "id": "2LJojW38fr5q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 39
        },
        "outputId": "fa1f1297-dec4-4c59-fb1c-57228a2331d5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <script>\n",
              "        let videoElement = null;\n",
              "        let stream = null;\n",
              "\n",
              "        async function startVideo() {\n",
              "            if (!videoElement) {\n",
              "                videoElement = document.createElement('video');\n",
              "                videoElement.setAttribute('autoplay', '');\n",
              "                videoElement.setAttribute('playsinline', '');\n",
              "                document.body.appendChild(videoElement);\n",
              "                stream = await navigator.mediaDevices.getUserMedia({ video: true })\n",
              "                  .catch(err => {\n",
              "                      console.error('Webcam not accessible:', err);\n",
              "                      alert('Webcam not accessible. You can upload an image instead.');\n",
              "                  });\n",
              "                if (stream) {\n",
              "                    videoElement.srcObject = stream;\n",
              "                }\n",
              "            }\n",
              "        }\n",
              "\n",
              "        async function capturePhoto() {\n",
              "            if (!videoElement) {\n",
              "                alert(\"Webcam is not active!\");\n",
              "                return;\n",
              "            }\n",
              "            const canvas = document.createElement('canvas');\n",
              "            canvas.width = videoElement.videoWidth;\n",
              "            canvas.height = videoElement.videoHeight;\n",
              "            canvas.getContext('2d').drawImage(videoElement, 0, 0);\n",
              "\n",
              "            // Stop video feed\n",
              "            stream.getTracks().forEach(track => track.stop());\n",
              "            videoElement.remove();\n",
              "            videoElement = null;\n",
              "\n",
              "            // Convert the photo to base64 and send to Python\n",
              "            const dataUrl = canvas.toDataURL('image/jpeg');\n",
              "            google.colab.kernel.invokeFunction('notebook.get_webcam_image', [dataUrl], {});\n",
              "        }\n",
              "\n",
              "        // Add the start and capture buttons to the DOM\n",
              "        const startButton = document.createElement('button');\n",
              "        startButton.innerHTML = 'Start Webcam Feed';\n",
              "        startButton.onclick = startVideo;\n",
              "        document.body.appendChild(startButton);\n",
              "\n",
              "        const captureButton = document.createElement('button');\n",
              "        captureButton.innerHTML = 'Capture Photo';\n",
              "        captureButton.onclick = capturePhoto;\n",
              "        document.body.appendChild(captureButton);\n",
              "    </script>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}